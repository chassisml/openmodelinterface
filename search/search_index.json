{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Open Model Interface (OMI) proposes a spec for multi-platform OCI-compatible container images for Machine Learning models. We believe that ML teams should be able to build reusable, DevOps-ready container images for interoperable consumption by multiple platforms to avoid being locked in to any one platform. Key points Models are baked into the container image, not downloaded at runtime Model serving framework can be switched at runtime by environment variable Pre- and post-processing logic can be shipped with the container image Spec includes recommendations around model management for reproducibility & provenance First reference implementation of OMI is available, Apache 2 licensed, at Chassis.ml Model Baking Including the model files in the container image itself is better than downloading them at runtime because it makes the model images self-contained, adheres to DevOps best practice, and improves reproducibility and reliability. In particular, some systems download a model from, say, an MLflow server when the container starts up. This relies on the MLflow server being online, which makes the MLflow server a critical part of your production infrastructure. As MLflow servers are often managed by ML teams, this is undesirable, and can result in downtime for your ML services if your MLflow server is unavailable when your model containers restart or scale up. In terms of reproducibility, baking models into containers is better because your CI/CD system can record the exact version of the container that was running at any given time, and so you can trace a specific inference result to the exact model that was running in production at that moment, and then trace it (see \"Origin tags\" in the spec) back to the exact model that trained it. Build once, run many By creating images that are \"multi-purpose\", you can avoid lock-in from a specific runtime platform. Instead, your ML teams can build container images that work in a variety of different platforms and are \"portable\" between different platforms with no effort. See the spec to see exactly how runtime configuration of the exposed interface works. Reference Implementation The first reference implementation of the OMI is available today, under the Apache 2 license, at Chassis.ml . There you can try a test drive of the reference implementation to play with it today. OMI Community Connections All things OMI are discussed in the \"Open Source\" channels of Modzy's Discord server. Anyone looking to understand more about OMI or influence the spec is welcome to join us over there. OMI Discord Roadmap Explainability support Drift support History The OMI was founded by Modzy a commercial ModelOps platform designed to run any kind of machine learning and artificial intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development as well as client feedback on production pipeline deployment. Modzy donated the best practices and lessons learned from its corporate ventures to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Introduction"},{"location":"#introduction","text":"The Open Model Interface (OMI) proposes a spec for multi-platform OCI-compatible container images for Machine Learning models. We believe that ML teams should be able to build reusable, DevOps-ready container images for interoperable consumption by multiple platforms to avoid being locked in to any one platform.","title":"Introduction"},{"location":"#key-points","text":"Models are baked into the container image, not downloaded at runtime Model serving framework can be switched at runtime by environment variable Pre- and post-processing logic can be shipped with the container image Spec includes recommendations around model management for reproducibility & provenance First reference implementation of OMI is available, Apache 2 licensed, at Chassis.ml","title":"Key points"},{"location":"#model-baking","text":"Including the model files in the container image itself is better than downloading them at runtime because it makes the model images self-contained, adheres to DevOps best practice, and improves reproducibility and reliability. In particular, some systems download a model from, say, an MLflow server when the container starts up. This relies on the MLflow server being online, which makes the MLflow server a critical part of your production infrastructure. As MLflow servers are often managed by ML teams, this is undesirable, and can result in downtime for your ML services if your MLflow server is unavailable when your model containers restart or scale up. In terms of reproducibility, baking models into containers is better because your CI/CD system can record the exact version of the container that was running at any given time, and so you can trace a specific inference result to the exact model that was running in production at that moment, and then trace it (see \"Origin tags\" in the spec) back to the exact model that trained it.","title":"Model Baking"},{"location":"#build-once-run-many","text":"By creating images that are \"multi-purpose\", you can avoid lock-in from a specific runtime platform. Instead, your ML teams can build container images that work in a variety of different platforms and are \"portable\" between different platforms with no effort. See the spec to see exactly how runtime configuration of the exposed interface works.","title":"Build once, run many"},{"location":"#reference-implementation","text":"The first reference implementation of the OMI is available today, under the Apache 2 license, at Chassis.ml . There you can try a test drive of the reference implementation to play with it today.","title":"Reference Implementation"},{"location":"#omi-community-connections","text":"All things OMI are discussed in the \"Open Source\" channels of Modzy's Discord server. Anyone looking to understand more about OMI or influence the spec is welcome to join us over there. OMI Discord","title":"OMI Community Connections"},{"location":"#roadmap","text":"Explainability support Drift support","title":"Roadmap"},{"location":"#history","text":"The OMI was founded by Modzy a commercial ModelOps platform designed to run any kind of machine learning and artificial intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development as well as client feedback on production pipeline deployment. Modzy donated the best practices and lessons learned from its corporate ventures to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"History"},{"location":"rationale/","text":"Rationale Why Open Model Interface? As machine learning and artificial intelligence become more wide-spread and with the rapid pace of innovation and invention there can often be barriers to entry in deploying models into production. New models need to undergo security evaluation and review and often times engineering effort needs to be scheduled to integrate those models into the systems, applications, and workflows where they will bring value. Various surveys indicate that this process can lead to a 6 month or greater delay between when a model is ready for production and when it's available for use in production. The Open Model Interface is designed to serve as a spec for wrapping models in OCI-compliant containers with a simple yet powerful interface that standardizes on certain security best practices, supports the widest range of machine learning and artificial intelligence training tools and frameworks, and enables re-use of existing integration code to add new or updated models to production environments by sharing a common API across all model containers. The spec itself tries to be as agnostic as possible as to what the model is and how it was trained. This is important because the world of machine learning and artificial intelligence is evolving so rapidly that any choices or limitations made now will inevitably become blockers to adoption in just a short amount of time. So the spec endeavors to make no assumptions about the language, framework, dependencies, or architectures of the model itself, save for its ability to be packaged as an OCI-compliant container image. The interface to the model as defined by the spec is a simple gRPC service that is embedded in the container image that exposes a small handful of routes to handle communication between users of the model and the model itself. Using a common gRPC interface to get status info about the model, submit data for inference, and initiate cleanup/shutdown sequence allows for a wide diversity of models to support existing and as-yet-not-invented techniques and architectures to be supported. Finally the Open Model Interface spec includes out-of-the-box support for deployment with popular ModelOps platforms so that models that adhere to this spec can be added to such platforms for increased scale, security, governance, and compliance. Models in production are rarely deployed alone or in a vacuum and while the Open Model Interface spec provides for a common API to interact and integrate with the model, it's important that models that adhere to this spec are not only useful in isolation but can be seamlessly used by platforms and tools that provide the full suite of ModelOps capabilities. Today the spec supports seamless deployment and usage with KFServing and Modzy with room to add support for more ModelOps platforms in the future. Why gRPC? The choice to use gRPC as the transport protocol for communication between users and the model is due to the wide flexibility in communication patterns that gRPC supports. While choosing JSON and REST presents a low barrier to entry, REST doesn't support more advanced and flexible communication patterns like uni- and bi-directional streaming which are crucial for many of today's more advanced models and usage patterns. The gRPC protocol is not only seeing increased usage in the world of cloud computing and microservices but it also supports all the communication patterns that are valuable for feeding data to models to perform inference. It supports single-shot data submission, batch, stream in, stream out, and bi-directional streams. Additionally, the ability to publish a protobuf file with the typed interface and RPC specification makes for extremely intuitive implementation for software developers wanting to integrate models into their applications and workflows. Using protobufs as the data encoding also increases efficiency and performance of data transmission which can make a real difference in high-performance and low-latency scenarios and reduce time and cost for data transmission over the wire for large applications of AI/ML, especially when running in the cloud. Why KFServing? The choice to include out-of-the-box support for KFService is due to the fact that it is open source and part of the widely used KubeFlow project. KFService provides an open source solution to running many Open Model Interface-compliant models in production with some additional ModelOps capabilities. Why Modzy? Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Rationale"},{"location":"rationale/#rationale","text":"","title":"Rationale"},{"location":"rationale/#why-open-model-interface","text":"As machine learning and artificial intelligence become more wide-spread and with the rapid pace of innovation and invention there can often be barriers to entry in deploying models into production. New models need to undergo security evaluation and review and often times engineering effort needs to be scheduled to integrate those models into the systems, applications, and workflows where they will bring value. Various surveys indicate that this process can lead to a 6 month or greater delay between when a model is ready for production and when it's available for use in production. The Open Model Interface is designed to serve as a spec for wrapping models in OCI-compliant containers with a simple yet powerful interface that standardizes on certain security best practices, supports the widest range of machine learning and artificial intelligence training tools and frameworks, and enables re-use of existing integration code to add new or updated models to production environments by sharing a common API across all model containers. The spec itself tries to be as agnostic as possible as to what the model is and how it was trained. This is important because the world of machine learning and artificial intelligence is evolving so rapidly that any choices or limitations made now will inevitably become blockers to adoption in just a short amount of time. So the spec endeavors to make no assumptions about the language, framework, dependencies, or architectures of the model itself, save for its ability to be packaged as an OCI-compliant container image. The interface to the model as defined by the spec is a simple gRPC service that is embedded in the container image that exposes a small handful of routes to handle communication between users of the model and the model itself. Using a common gRPC interface to get status info about the model, submit data for inference, and initiate cleanup/shutdown sequence allows for a wide diversity of models to support existing and as-yet-not-invented techniques and architectures to be supported. Finally the Open Model Interface spec includes out-of-the-box support for deployment with popular ModelOps platforms so that models that adhere to this spec can be added to such platforms for increased scale, security, governance, and compliance. Models in production are rarely deployed alone or in a vacuum and while the Open Model Interface spec provides for a common API to interact and integrate with the model, it's important that models that adhere to this spec are not only useful in isolation but can be seamlessly used by platforms and tools that provide the full suite of ModelOps capabilities. Today the spec supports seamless deployment and usage with KFServing and Modzy with room to add support for more ModelOps platforms in the future.","title":"Why Open Model Interface?"},{"location":"rationale/#why-grpc","text":"The choice to use gRPC as the transport protocol for communication between users and the model is due to the wide flexibility in communication patterns that gRPC supports. While choosing JSON and REST presents a low barrier to entry, REST doesn't support more advanced and flexible communication patterns like uni- and bi-directional streaming which are crucial for many of today's more advanced models and usage patterns. The gRPC protocol is not only seeing increased usage in the world of cloud computing and microservices but it also supports all the communication patterns that are valuable for feeding data to models to perform inference. It supports single-shot data submission, batch, stream in, stream out, and bi-directional streams. Additionally, the ability to publish a protobuf file with the typed interface and RPC specification makes for extremely intuitive implementation for software developers wanting to integrate models into their applications and workflows. Using protobufs as the data encoding also increases efficiency and performance of data transmission which can make a real difference in high-performance and low-latency scenarios and reduce time and cost for data transmission over the wire for large applications of AI/ML, especially when running in the cloud.","title":"Why gRPC?"},{"location":"rationale/#why-kfserving","text":"The choice to include out-of-the-box support for KFService is due to the fact that it is open source and part of the widely used KubeFlow project. KFService provides an open source solution to running many Open Model Interface-compliant models in production with some additional ModelOps capabilities.","title":"Why KFServing?"},{"location":"rationale/#why-modzy","text":"Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Why Modzy?"},{"location":"spec/","text":"Open Model Interface Spec The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119 . Changelog Date Version Notes 2021-07-26 1.0 Initial version of OMI Spec 2022-03-24 1.1 Updated OMI Spec to include new requirements Requirements To qualify as an Open Model Interface (OMI) compliant container, a container MUST pass the following tests. OCI compliant The container image MUST be an OCI compliant container image, for example it MAY be generated by docker build or Kaniko . Origin tags The container MUST be tagged (e.g. tag in registry/org/image:tag ) with a unique ID which corresponds to the ID of the model in the upstream model management system. For example, the container image tag MAY be set to the MLflow Run ID which generated the model. This is to enable provenance tracking from a given model which generated certain inference results to metadata about how the model was trained. The metadata referred to by the model tag SHOULD record the versions of the code, data and environment that were used to train the model. Declares Openmodel Annotations The container MUST have OCI annotations of the following form: ml.openmodel.interfaces=[\"kfserving\", \"modzy\"] ml.openmodel.protocols=[[\"v1\",\"v2\"],[\"v2\"]] ml.openmodel.port=\"8080\" ml.openml.model_name=\"modelname\" Where ml.openmodel.protocol is list of lists such that the inner list's index corresponds to its interface index e.g. a model with the above configuration would support kfserving v1 and v2 protocols as well as the modzy interface with v2 protocol. This is so that the serving system at runtime can determine whether it supports a given model without trying to run it so that it can give the user fast feedback. Ports When started, the container image MUST listen on the TCP port given as the ml.openmodel.port annotation. When a container starts listening on the prescribed port, it MUST adhere to the interface described below corresponding to its configuration. Environment variables OCI annotations are not available in a running Docker container. As a result, an OMI container MUST be configurable with the following environment variables to provide a copy of the values associated with the OMI labels: env : - name : INTERFACE value : <ml.openmodel.interfaces> # e.g. kfserving - name : HTTP_PORT value : <ml.openmodel.port> # e.g. 8080 - name : PROTOCOL value : <ml.openmodel.protocols> # e.g. v2 - name : MODEL_NAME value : <ml.openml.model_name> # e.g. mdodel name INTERFACE MUST be the string interface supported, as defined in the interfaces section below. PROTOCOL MAY be the optional sub-protocol of the given interface MODEL_NAME MUST be some human-readable name of the model that is to be run inside the container. HTTP_PORT MUST be the TCP port that the container should listen on when started If a conflict arises between the OCI annotations and the environment variables, The OMI specification considers OCI annotations to be authoritative. Interfaces and Protocols The following interfaces are permitted in the spec: kfserving : Supports v1 REST API with ml.openmodel.interfaces=[\"kfserving\"] and ml.openmodel.protocols=\"v1\" v2 gRPC API with ml.openmodel.interfaces=[\"kfserving\"] and ml.openmodel.protocols=\"v2\" modzy : Supports v1 REST API with ml.openmodel.interfaces=[\"modzy\"] and ml.openmodel.protocols=\"v1\" v2 gRPC API with ml.openmodel.interfaces=[\"modzy\"] and ml.openmodel.protocols=\"v2\" Compliant Implementations OMI compliant container images MUST: * implement at least one of the kfserving or modzy API interfaces * be runnable and responsive on both KServe and Modzy platforms * be runnable independently in isolation For example: The Chassis reference implementation of the OMI specification produces containers * that implement the modzy.v2 API * that implement a KServe compatible architecture that follows the kfserving.v1 specification with one exception. Chassis-generated model containers expect binary input data instead of the json input data the kfserving.v1 spec designates. Therefore, for KFServing users must base64-encode their input bytes to be included within their json input submission. In this way, the model containers generated by Chassis will run on KFServing as well as Modzy. * that can run in isolation Updates to this spec If an OMI image declares or supports any strings other than the ones defined above, this document MUST be updated with the new interfaces (with links to their upstream protocol specifications), otherwise the container MUST NOT be described as Open Model Interface compliant. This document can be updated via Pull Request on GitHub . Any proposed PRs must be accompanied by a public comment period of no less than 2 weeks (14 days) before a merge can be approved. Contributions welcome!","title":"Open Model Interface Spec"},{"location":"spec/#open-model-interface-spec","text":"The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119 .","title":"Open Model Interface Spec"},{"location":"spec/#changelog","text":"Date Version Notes 2021-07-26 1.0 Initial version of OMI Spec 2022-03-24 1.1 Updated OMI Spec to include new requirements","title":"Changelog"},{"location":"spec/#requirements","text":"To qualify as an Open Model Interface (OMI) compliant container, a container MUST pass the following tests.","title":"Requirements"},{"location":"spec/#oci-compliant","text":"The container image MUST be an OCI compliant container image, for example it MAY be generated by docker build or Kaniko .","title":"OCI compliant"},{"location":"spec/#origin-tags","text":"The container MUST be tagged (e.g. tag in registry/org/image:tag ) with a unique ID which corresponds to the ID of the model in the upstream model management system. For example, the container image tag MAY be set to the MLflow Run ID which generated the model. This is to enable provenance tracking from a given model which generated certain inference results to metadata about how the model was trained. The metadata referred to by the model tag SHOULD record the versions of the code, data and environment that were used to train the model.","title":"Origin tags"},{"location":"spec/#declares-openmodel-annotations","text":"The container MUST have OCI annotations of the following form: ml.openmodel.interfaces=[\"kfserving\", \"modzy\"] ml.openmodel.protocols=[[\"v1\",\"v2\"],[\"v2\"]] ml.openmodel.port=\"8080\" ml.openml.model_name=\"modelname\" Where ml.openmodel.protocol is list of lists such that the inner list's index corresponds to its interface index e.g. a model with the above configuration would support kfserving v1 and v2 protocols as well as the modzy interface with v2 protocol. This is so that the serving system at runtime can determine whether it supports a given model without trying to run it so that it can give the user fast feedback.","title":"Declares Openmodel Annotations"},{"location":"spec/#ports","text":"When started, the container image MUST listen on the TCP port given as the ml.openmodel.port annotation. When a container starts listening on the prescribed port, it MUST adhere to the interface described below corresponding to its configuration.","title":"Ports"},{"location":"spec/#environment-variables","text":"OCI annotations are not available in a running Docker container. As a result, an OMI container MUST be configurable with the following environment variables to provide a copy of the values associated with the OMI labels: env : - name : INTERFACE value : <ml.openmodel.interfaces> # e.g. kfserving - name : HTTP_PORT value : <ml.openmodel.port> # e.g. 8080 - name : PROTOCOL value : <ml.openmodel.protocols> # e.g. v2 - name : MODEL_NAME value : <ml.openml.model_name> # e.g. mdodel name INTERFACE MUST be the string interface supported, as defined in the interfaces section below. PROTOCOL MAY be the optional sub-protocol of the given interface MODEL_NAME MUST be some human-readable name of the model that is to be run inside the container. HTTP_PORT MUST be the TCP port that the container should listen on when started If a conflict arises between the OCI annotations and the environment variables, The OMI specification considers OCI annotations to be authoritative.","title":"Environment variables"},{"location":"spec/#interfaces-and-protocols","text":"The following interfaces are permitted in the spec: kfserving : Supports v1 REST API with ml.openmodel.interfaces=[\"kfserving\"] and ml.openmodel.protocols=\"v1\" v2 gRPC API with ml.openmodel.interfaces=[\"kfserving\"] and ml.openmodel.protocols=\"v2\" modzy : Supports v1 REST API with ml.openmodel.interfaces=[\"modzy\"] and ml.openmodel.protocols=\"v1\" v2 gRPC API with ml.openmodel.interfaces=[\"modzy\"] and ml.openmodel.protocols=\"v2\"","title":"Interfaces and Protocols"},{"location":"spec/#compliant-implementations","text":"OMI compliant container images MUST: * implement at least one of the kfserving or modzy API interfaces * be runnable and responsive on both KServe and Modzy platforms * be runnable independently in isolation For example: The Chassis reference implementation of the OMI specification produces containers * that implement the modzy.v2 API * that implement a KServe compatible architecture that follows the kfserving.v1 specification with one exception. Chassis-generated model containers expect binary input data instead of the json input data the kfserving.v1 spec designates. Therefore, for KFServing users must base64-encode their input bytes to be included within their json input submission. In this way, the model containers generated by Chassis will run on KFServing as well as Modzy. * that can run in isolation","title":"Compliant Implementations"},{"location":"spec/#updates-to-this-spec","text":"If an OMI image declares or supports any strings other than the ones defined above, this document MUST be updated with the new interfaces (with links to their upstream protocol specifications), otherwise the container MUST NOT be described as Open Model Interface compliant. This document can be updated via Pull Request on GitHub . Any proposed PRs must be accompanied by a public comment period of no less than 2 weeks (14 days) before a merge can be approved. Contributions welcome!","title":"Updates to this spec"}]}